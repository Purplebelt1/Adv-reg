
```{r load_packages2, message = FALSE}
# Packages required for Chapter 2
library(gridExtra)  
library(knitr) 
library(mosaic)
library(xtable)
library(kableExtra)
library(tidyverse) 
```

```{r include=FALSE}
if(knitr::is_html_output()){options(knitr.table.format = "html")} else {options(knitr.table.format = "latex")}
```

```{r, logistic1, fig.align="center",out.width="90%", fig.cap='An attempt to fit a linear regression model to a binary response variable.',echo=FALSE, warning=FALSE, message=FALSE}
time <- c(2,2,3,3,4,5,5,6,6,7,7,7,8,9,10,11,11,12,12,14)
correct <- c(0,0,1,0,1,0,0,1,1,0,0,1,1,1,0,1,1,1,0,1)
testdf <- data.frame(time, correct)
testplot<- ggplot(data = testdf,aes(x = time, y = correct)) + 
  geom_point() + geom_smooth(method = "lm") + 
  xlab("Hours spent studying") + ylab("Correct (0 = no, 1 = yes)")  
testplot
```

## Exercises
### Conceptual Exercises


### Question 1 
1. Suppose we plan to use data to estimate one parameter, $p_B$. 
    - When using a likelihood to obtain an estimate for the parameter, which is preferred: a large or a small likelihood value? Why?
    <br>
    A large likelihood value is preferred when estimating a parameter, because it means that the observed data is more probable under the assumed model and parameter value


### Question 2
2. Suppose the families with an "only child" were excluded for the Sex Conditional Model. How might the estimates for the three parameters be affected?  Would it still be possible to perform a Likelihood Ratio Test to compare the Sex Unconditional and Sex Conditional Models?  Why or why not?
<br>
For conditional singleton families greatly help estimate Pb|N. Removing singleton families would not affect Pb|b_{bias} Pb|g_{bias} though. For unconditional models it would make the sample size smaller worsening the parameter estimate. It would still be possbile to run a Likelihood Ratio Test if both models had no singleton families as the Likelihood Ratio Test compares the likelihoods of the two models for the same data.
    
### Question 3
    
1. Write out the likelihood for a model which assumes the probability of a girl equals the probability of a boy. Carry out a LRT to determine whether there is evidence that the two probabilities are not equal. Comment on the practical significance of this finding (there is not necessarily one correct answer).

```{R}
# Apply Model 1 to NLSY data (for families with 3 or fewer
# children)
# possible values for prob a boy is born
pb <- seq(0, 1, length = 10001)
# loglik of getting obs data
loglik <- 5416 * log(pb) + 5256 * log(1 - pb)
# maximum loglikelihood over all values of pb
max(loglik)
# MLE of pb
pb[loglik==max(loglik)]
max_logL_m1_nlsy <- max(loglik)
# Model 0 is Model 1 with pb set equal to 0.5
maxloglik0 <- 5416 * log(0.5) + 5256 * log(1 - 0.5)
maxloglik0

# lrt
lrt <- 2 * (max_logL_m1_nlsy - maxloglik0)
lrt

# Find P val
pval <- 1 - pchisq(lrt, df = 1)
pval
```

Since p value is greater than .05 we do not reject the null hypothesis that the probability of a not is the same as the probabilty of a girl.

### Question 4
2. __Case 3__ In Case 1 we used hypothetical data with 30 boys and 20 girls. Case 2 was a much larger study with 600 boys and 400 girls. Consider Case 3, a hypothetical data set with 6000 boys and 4000 girls.  
    - Use the methods for Case 1 and Case 2 and determine the MLE for $p_B$ for the independence model. Compare your result to the MLEs for Cases 1 and 2.
    - Describe how the graph of the log-likelihood for Case 3 would compare to the log-likelihood graphs for Cases 1 and 2.
    - Compute the log-likelihood for Case 3. Why is it incorrect to perform an LRT comparing Cases 1, 2, and 3?
    
```{r, include=FALSE}
p = seq(0,1,.01)   # possible values for p
#
# Defining a function that calculates logLikelihoods
logLik <- function(n.boys,n.girls){
  Lik=p^n.boys*(1-p)^n.girls
  logLik=log(Lik)
  lik.frame <- data.frame(p,Lik,logLik)
  return(lik.frame)
}

#define data frames
one.frame <- logLik(30,20)
two.frame <- logLik(600,400)
three.frame <- logLik(6000,4000)

# 4 graphs for 2 x 2 plot
one.log <- ggplot(one.frame,aes(x=p,y=logLik)) + geom_line() + 
  labs(x="p",y="logLikelihood",title="(case 1) \n30 boys, 20 girls log(Likelihood)") +
 theme(axis.ticks = element_blank(), axis.text.y = element_blank())

two.log <- ggplot(two.frame,aes(x=p,y=logLik)) + geom_line() + 
  labs(x="p",y="logLikelihood",title="(case 2) \n600 boys, 400 girls log(Likelihood)") +
 theme(axis.ticks = element_blank(), axis.text.y = element_blank())

three.log <- ggplot(three.frame,aes(x=p,y=logLik)) + geom_line() + 
  labs(x="p",y="logLikelihood",title="(case 3) \n6000 boys, 4000 girls log(Likelihood)") +
 theme(axis.ticks = element_blank(), axis.text.y = element_blank())

```

```{r, lik4, fig.align="center",out.width="90%", fig.cap='Likelihood and log-likelihood functions for 50 children (30 boys and 20 girls), for 1000 children (600 boys and 400 girls) and for 10000 children (6000 boys and 4000 girls).',echo=FALSE, warning=FALSE, message=FALSE}
grid.arrange(one.log,two.log,three.log,ncol=1)
```


 - MLE for pb is just 6000/10000 or .6 the same as the MLE for case 1 and case 2.
 <br>
 - The graph for loglik for case 3 would have a much sharper spike at .6 then the other cases.
 <br>
 - Loglik for case 3 would be LogL(.6)= 6000log(.6) + 4000log(.4) = -2922.85253239. Cant do the LTR due to the fact that they are using differnt data. LTR can only be done when the models share exact same data.
 


3. Write out an expression for the likelihood of seeing our NLSY data (5,416 boys and 5,256 girls) if the true probability of a boy is:
    (a) $p_B=0.5$  
    (b) $p_B=0.45$  
    - Compute the value of the  log-likelihood for each of the values of $p_B$ above.
    - Which of these four possibilities, $p_B=0.45, p_B=0.5,  p_B=0.55,$ or $p_B=0.5075$ would be the best estimate
      of $p_B$ given what we observed (our data)?
      
```{R}
# Define the observed data
boys <- 5416
girls <- 5256

# Possible values for prob a boy is born
pb_values <- c(0.45, 0.5, 0.55, 0.5075)

# Initialize vectors to store log-likelihood values
loglik_values <- numeric(length(pb_values))

# Calculate log-likelihood for each value of pb
for (i in seq_along(pb_values)) {
  pb <- pb_values[i]
  loglik_values[i] <- boys * log(pb) + girls * log(1 - pb)
}

# Display log-likelihood values
loglik_values

# Find the index of the maximum log-likelihood
best_estimate_index <- which.max(loglik_values)

# Display the best estimate of pb
best_estimate_pb <- pb_values[best_estimate_index]
best_estimate_pb

```
See code output. First output is all loglik and 2nd is best loglik


4. Compare the Waiting for a Boy Model to a Random Stopping Model. The parameters for the first model (Waiting for a Boy) are $p_B$, $\bstop$, $\nstop$ and the parameters for the second model (Random Stopping) are $p_B$ and $p_S$. Use an intuitive approach to arrive at the MLEs for the parameters for each model. Perform a LRT to compare these two models. 

### Open-Ended Exercises
    

2. **The hot hand in basketball.**  @Gilovich1985 wrote a controversial but compelling article claiming that there is no such thing as “the hot hand” in basketball.  That is, there is no empirical evidence that shooters have stretches where they are more likely to make consecutive shots, and basketball shots are essentially independent events.  One of the many ways they tested for evidence of a “hot hand” was to record sequences of shots for players under game conditions and determine if players are more likely to make shots after made baskets than after misses.  For instance, assume we recorded data from one player's first 5 three-point attempts over a 5-game period.  We can assume games are independent, but we’ll consider two models for shots within a game:

 - No Hot Hand (1 parameter): $p_B$ = probability of making a basket (thus $1-p_B$ = probability of not making a basket).

    - Hot Hand (2 parameters): $p_B$ = probability of making a basket after a miss (or the first shot of a game); $p_{B|B}$ = probability of making a basket after making the previous shot.


    a. Fill out Table \@ref(tab:hothandchp2)---write out the contribution of each game to the likelihood for both models along with the total likelihood for each model.

    b. Given that, for the No Hot Hand model, $\textrm{Lik}(p_B)=p_B^{10}(1-p_B)^{15}$ for the 5 games where we collected data, how do we know that 0.40 (the maximum likelihood estimator (MLE) of $p_B$) is a better estimate than, say, 0.30?

    c. Find the MLEs for the parameters in each model, and then use those MLEs to determine if there's significant evidence that the hot hand exists.


```{r, include=FALSE}
Game <- c(1,2,3,4,5, "Total")
First5 <- c("BMMBB", "MBMBM", "MMBBB", "BMMMB", "MMMMM", " ")
NoHotHand <- c(" "," "," "," "," "," ")
HotHand <- c(" "," "," "," "," "," ")
```
    
```{r, hothandchp2, echo=FALSE}
hothandchp2 <- data.frame(Game, First5, NoHotHand, HotHand)
kable(hothandchp2, booktabs = T, 
      col.names = c("Game", "First 5 shots", 
                    "Likelihood (No Hot Hand)", 
                    "Likelihood (Hot Hand)"),
      caption = "Data for Open-ended Exercise 2.  (B = made basket.  M = missed basket.)") %>% 
  column_spec(1:4, width = "2cm") %>% 
  kable_styling(latex_options = "scale_down")
```

```{R}

count_overlapping_occurrences <- function(text, pattern) {
  count <- 0
  for (i in 1:(nchar(text) - nchar(pattern) + 1)) {
    substring <- substr(text, i, i + nchar(pattern) - 1)
    if (substring == pattern) {
      count <- count + 1
    }
  }
  return(count)
}

for (i in 1:5) {
  shots <- strsplit(as.character(hothandchp2$First5[i]), "")[[1]]
  
  num_makes <- sum(shots == "B")
  num_misses <- sum(shots == "M")
  
  num_BB <- count <- count_overlapping_occurrences(hothandchp2$First5[i], "BB")
  num_BM <- count <- str_count(hothandchp2$First5[i], "BM")
  
  hothandchp2$NoHotHand[i] <- paste0("p^", num_makes, "_B (1 - p_B)^", num_misses)

   hothandchp2$HotHand[i] <- paste0("p^", num_makes - num_BB, "_B (1 - p_B)^", num_misses - num_BM, 
                                     "p^", num_BB, "_B|B (1 - p_B|B)^", num_BM)
}

hothandchp2$NoHotHand[6] <- "p^10_B (1 - p_B)^15"
hothandchp2$HotHand[6] <- "p^7_B (1 - p_B)^1p^3-1_B|B (1 - p_B|B)^4"

print(hothandchp2)
```


```{R}
likelihood_no_hot_hand_model <- function(p_B) {
  return(p_B^10 * (1 - p_B)^15)
}

likelihood_0.40 <- likelihood_no_hot_hand_model(0.40)

likelihood_0.30 <- likelihood_no_hot_hand_model(0.30)

cat("Likelihood for p_B = 0.40:", likelihood_0.40, "\n")
cat("Likelihood for p_B = 0.30:", likelihood_0.30, "\n")

```

Likelihood for .40 is better.


```{R}
likelihood_no_hot_hand_model <- function(p_B) {
  return(p_B^10 * (1 - p_B)^15)
}
log_likelihood_no_hot_hand_model <- function(p_B) {
  return(10 * log(p_B) + 15 * log(1 - p_B))
}
mle_result <- optim(par = 0.5, fn = function(p) -log_likelihood_no_hot_hand_model(p), method = "Brent", lower = 0, upper = 1)
mle_p_B <- mle_result$par
mle_likelihood <- likelihood_no_hot_hand_model(mle_p_B)
cat("MLE for p_B:", mle_p_B, "\n")
cat("Likelihood at MLE:", mle_likelihood, "\n")

```




